{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lakehouse Architecture Demo\n",
        "\n",
        "This notebook demonstrates key features of the Lakehouse Architecture:\n",
        "- Bronze, Silver, and Gold layers\n",
        "- Delta Lake features (Time Travel, ACID transactions)\n",
        "- Data quality checks\n",
        "- Querying and analytics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "from config.spark_config import create_spark_session, get_table_paths\n",
        "from pyspark.sql.functions import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Spark session\n",
        "spark, base_path = create_spark_session(app_name=\"LakehouseDemo\")\n",
        "paths = get_table_paths(base_path)\n",
        "\n",
        "print(f\"Base path: {base_path}\")\n",
        "print(f\"Bronze path: {paths['bronze']}\")\n",
        "print(f\"Silver path: {paths['silver']}\")\n",
        "print(f\"Gold path: {paths['gold']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bronze Layer - Raw Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 5: Delta Lake Features - Time Travel\n",
        "\n",
        "Query historical versions of data using version number or timestamp.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Travel: Query table history\n",
        "from scripts.delta_features.delta_lake_features import get_table_history, time_travel_by_version\n",
        "\n",
        "# Show version history\n",
        "get_table_history(spark, f\"{paths['bronze']}/transactions\")\n",
        "\n",
        "# Query version 0 (oldest/first load)\n",
        "# historical_df = time_travel_by_version(spark, f\"{paths['bronze']}/transactions\", 0)\n",
        "# historical_df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema Evolution\n",
        "\n",
        "Add new columns without breaking existing queries. Delta Lake supports `mergeSchema` option.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema evolution example - add new columns with defaults\n",
        "# df_with_new_cols = bronze_transactions.withColumn(\"data_quality_score\", lit(100))\n",
        "# df_with_new_cols.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read Bronze transactions\n",
        "bronze_transactions = spark.read.format(\"delta\").load(f\"{paths['bronze']}/transactions\")\n",
        "\n",
        "print(\"Bronze Transactions Schema:\")\n",
        "bronze_transactions.printSchema()\n",
        "\n",
        "print(\"\\nSample Records:\")\n",
        "bronze_transactions.show(5)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
