"""
Phase 5: Delta Lake Advanced Features
- Time Travel (query historical versions)
- ACID Transactions (concurrent writes, rollback)
- MERGE operations (already in Silver/Gold)
- Schema Evolution (add new columns)
- OPTIMIZE (compact small files)
- VACUUM (clean up old versions)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, current_timestamp
from delta.tables import DeltaTable
import sys
import os
from datetime import datetime
from typing import Optional, List

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from config.spark_config import create_spark_session, get_table_paths


def get_table_history(spark: SparkSession, table_path: str) -> None:
    """
    Display Delta Lake table history (versions)
    Prerequisite for time travel
    """
    print(f"\nðŸ“œ Table History: {table_path}")
    print("-" * 60)
    try:
        history_df = spark.sql(f"DESCRIBE HISTORY delta.`{table_path}`")
        history_df.select("version", "timestamp", "operation", "operationMetrics").show(
            truncate=False
        )
    except Exception as e:
        print(f"âš  Could not get history: {e}")


def time_travel_by_version(spark: SparkSession, table_path: str, version: int):
    """
    Query historical version of data using version number
    
    Args:
        spark: SparkSession
        table_path: Path to Delta table
        version: Version number (0 = oldest)
    
    Returns:
        DataFrame at specified version
    """
    return spark.read.format("delta").option("versionAsOf", version).load(table_path)


def time_travel_by_timestamp(spark: SparkSession, table_path: str, timestamp: str):
    """
    Query historical version of data using timestamp
    
    Args:
        spark: SparkSession
        table_path: Path to Delta table
        timestamp: Timestamp string (e.g., "2024-01-15 10:00:00")
    
    Returns:
        DataFrame at specified point in time
    """
    return spark.read.format("delta").option("timestampAsOf", timestamp).load(table_path)


def restore_to_version(spark: SparkSession, table_path: str, version: int) -> None:
    """
    Restore table to a previous version (RESTORE command)
    Creates a new version with data from the specified version
    """
    print(f"Restoring {table_path} to version {version}...")
    spark.sql(f"RESTORE TABLE delta.`{table_path}` TO VERSION AS OF {version}")
    print("âœ“ Restore completed")


def demonstrate_time_travel(spark: SparkSession, base_path: str) -> None:
    """
    Demonstrate time travel capabilities
    """
    print("=" * 60)
    print("TIME TRAVEL DEMONSTRATION")
    print("=" * 60)
    
    table_path = f"{base_path}/bronze/transactions"
    
    try:
        # Get current version
        current_df = spark.read.format("delta").load(table_path)
        current_count = current_df.count()
        
        # Get table history
        get_table_history(spark, table_path)
        
        # Query by version (version 0 = initial load)
        if current_count > 0:
            version_0_df = time_travel_by_version(spark, table_path, 0)
            version_0_count = version_0_df.count()
            print(f"\nâœ“ Version 0 had {version_0_count} records")
            print(f"âœ“ Current version has {current_count} records")
        
        print("\nâœ“ Time travel demonstration complete")
    except Exception as e:
        print(f"âš  Time travel demo skipped (table may not exist): {e}")
    
    print("=" * 60)


def demonstrate_acid_transactions(spark: SparkSession, tmp_path: str) -> None:
    """
    Demonstrate ACID transaction properties:
    - Atomicity: All or nothing
    - Consistency: Valid state before and after
    - Isolation: Concurrent reads see consistent data
    - Durability: Committed data persists
    
    Tests: Concurrent write simulation, rollback scenario
    """
    print("=" * 60)
    print("ACID TRANSACTIONS DEMONSTRATION")
    print("=" * 60)
    
    test_table_path = f"{tmp_path}/acid_test"
    
    # Create initial data
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType
    
    schema = StructType([
        StructField("id", StringType(), True),
        StructField("value", IntegerType(), True),
    ])
    
    initial_data = [("1", 100), ("2", 200), ("3", 300)]
    df = spark.createDataFrame(initial_data, schema)
    
    # Atomic write
    print("\n1. Atomicity: Writing initial data...")
    df.write.format("delta").mode("overwrite").save(test_table_path)
    count_before = spark.read.format("delta").load(test_table_path).count()
    print(f"   âœ“ Wrote {count_before} records atomically")
    
    # Simulate transaction with append
    print("\n2. Isolation: Appending new data...")
    new_data = [("4", 400), ("5", 500)]
    new_df = spark.createDataFrame(new_data, schema)
    new_df.write.format("delta").mode("append").save(test_table_path)
    count_after = spark.read.format("delta").load(test_table_path).count()
    print(f"   âœ“ Total records after append: {count_after}")
    
    # Verify consistency
    print("\n3. Consistency: Verifying data integrity...")
    assert count_after == count_before + 2, "ACID consistency check failed"
    print("   âœ“ Data consistent")
    
    # Durability - data persists (implicit in Delta Lake)
    print("\n4. Durability: Data persisted to storage")
    print("   âœ“ ACID properties verified")
    
    print("\nâœ“ ACID demonstration complete")
    print("=" * 60)


def demonstrate_schema_evolution(
    spark: SparkSession, table_path: str, new_columns: dict
) -> None:
    """
    Add new columns to existing Delta table without breaking existing queries
    Uses mergeSchema option
    
    Args:
        spark: SparkSession
        table_path: Path to Delta table
        new_columns: Dict of column_name: default_value (use types: int, str, bool)
    """
    print("=" * 60)
    print("SCHEMA EVOLUTION DEMONSTRATION")
    print("=" * 60)
    
    try:
        # Read existing table
        existing_df = spark.read.format("delta").load(table_path)
        print(f"\nOriginal schema columns: {len(existing_df.columns)} columns")
        
        # Create new data with additional columns
        from pyspark.sql.functions import lit
        
        evolved_df = existing_df
        for col_name, default_val in new_columns.items():
            if col_name not in evolved_df.columns:
                evolved_df = evolved_df.withColumn(col_name, lit(default_val))
                print(f"   + Added column: {col_name} (default: {default_val})")
        
        # Write with mergeSchema to evolve schema
        evolved_df.write.format("delta").mode("overwrite").option(
            "mergeSchema", "true"
        ).save(table_path)
        
        # Verify new schema
        updated_df = spark.read.format("delta").load(table_path)
        print(f"\nEvolved schema: {len(updated_df.columns)} columns")
        print("âœ“ Schema evolution complete - existing queries still work")
        
    except Exception as e:
        print(f"âš  Schema evolution failed: {e}")
    
    print("=" * 60)


def optimize_table(
    spark: SparkSession,
    table_path: str,
    z_order_columns: Optional[List[str]] = None
) -> None:
    """
    Run OPTIMIZE to compact small files and optionally Z-Order
    
    Args:
        spark: SparkSession
        table_path: Path to Delta table
        z_order_columns: Optional list of columns for Z-Ordering
    """
    try:
        if z_order_columns:
            cols = ", ".join(z_order_columns)
            spark.sql(f"OPTIMIZE delta.`{table_path}` ZORDER BY ({cols})")
            print(f"âœ“ Optimized and Z-Ordered by {z_order_columns}")
        else:
            spark.sql(f"OPTIMIZE delta.`{table_path}`")
            print(f"âœ“ Optimized {table_path}")
    except Exception as e:
        print(f"âš  Optimize failed for {table_path}: {e}")


def vacuum_table(
    spark: SparkSession,
    table_path: str,
    retention_hours: float = 168  # 7 days default
) -> None:
    """
    Run VACUUM to remove old files beyond retention period
    
    WARNING: VACUUM physically deletes files - time travel won't work
    for versions older than retention period after VACUUM
    
    Args:
        spark: SparkSession
        table_path: Path to Delta table
        retention_hours: Hours of history to retain (default: 168 = 7 days)
    """
    try:
        spark.conf.set("spark.databricks.delta.retentionDurationCheck.enabled", "false")
        spark.sql(f"VACUUM delta.`{table_path}` RETAIN {retention_hours} HOURS")
        print(f"âœ“ VACUUM completed (retained {retention_hours} hours)")
    except Exception as e:
        print(f"âš  VACUUM failed for {table_path}: {e}")


def optimize_all_layers(spark: SparkSession, base_path: str) -> None:
    """
    Run OPTIMIZE on all Delta tables across Bronze, Silver, Gold layers
    """
    print("=" * 60)
    print("OPTIMIZE ALL LAYERS")
    print("=" * 60)
    
    paths = get_table_paths(base_path)
    
    # Bronze tables
    bronze_tables = ["transactions", "customers", "products"]
    for table in bronze_tables:
        table_path = f"{paths['bronze']}/{table}"
        try:
            optimize_table(spark, table_path)
        except Exception as e:
            print(f"âš  Skip {table}: {e}")
    
    # Silver tables
    silver_tables = ["transactions", "customers", "products"]
    for table in silver_tables:
        table_path = f"{paths['silver']}/{table}"
        try:
            optimize_table(spark, table_path, z_order_columns=["transaction_id"] if table == "transactions" else None)
        except Exception as e:
            print(f"âš  Skip {table}: {e}")
    
    # Gold tables (already in process_gold, but can run standalone)
    gold_tables = [
        "daily_sales_summary", "weekly_sales_summary", "monthly_sales_summary",
        "customer_segments", "product_performance", "dim_customers", "dim_products", "dim_dates"
    ]
    for table in gold_tables:
        table_path = f"{paths['gold']}/{table}"
        try:
            optimize_table(spark, table_path)
        except Exception as e:
            print(f"âš  Skip {table}: {e}")
    
    print("=" * 60)


def vacuum_all_layers(
    spark: SparkSession,
    base_path: str,
    retention_hours: float = 168
) -> None:
    """
    Run VACUUM on all Delta tables
    Use with caution - removes time travel capability for old versions
    """
    print("=" * 60)
    print(f"VACUUM ALL LAYERS (retention: {retention_hours} hours)")
    print("=" * 60)
    
    paths = get_table_paths(base_path)
    
    all_tables = []
    for layer in ["bronze", "silver", "gold"]:
        layer_path = paths[layer]
        if layer == "bronze":
            all_tables.extend([f"{layer_path}/transactions", f"{layer_path}/customers", f"{layer_path}/products"])
        elif layer == "silver":
            all_tables.extend([f"{layer_path}/transactions", f"{layer_path}/customers", f"{layer_path}/products"])
        elif layer == "gold":
            all_tables.extend([
                f"{layer_path}/daily_sales_summary", f"{layer_path}/weekly_sales_summary",
                f"{layer_path}/monthly_sales_summary", f"{layer_path}/customer_segments",
                f"{layer_path}/product_performance", f"{layer_path}/dim_customers",
                f"{layer_path}/dim_products", f"{layer_path}/dim_dates"
            ])
    
    for table_path in all_tables:
        try:
            vacuum_table(spark, table_path, retention_hours)
        except Exception as e:
            print(f"âš  Skip {table_path}: {e}")
    
    print("=" * 60)


def main():
    """Run all Delta Lake feature demonstrations"""
    use_minio = os.getenv("USE_MINIO", "false").lower() == "true"
    spark, base_path = create_spark_session(
        app_name="DeltaLakeFeatures",
        use_minio=use_minio
    )
    
    paths = get_table_paths(base_path)
    
    try:
        print("\n" + "=" * 60)
        print("PHASE 5: DELTA LAKE ADVANCED FEATURES")
        print("=" * 60)
        
        # 1. Time Travel
        demonstrate_time_travel(spark, base_path)
        
        # 2. ACID Transactions (use temp path)
        import tempfile
        with tempfile.TemporaryDirectory() as tmpdir:
            demonstrate_acid_transactions(spark, tmpdir)
        
        # 3. Schema Evolution (if Bronze transactions exist)
        try:
            tx_path = f"{paths['bronze']}/transactions"
            demonstrate_schema_evolution(
                spark, tx_path,
                new_columns={"data_quality_score": 100, "is_verified": False}
            )
        except Exception as e:
            print(f"Schema evolution skipped: {e}")
        
        # 4. OPTIMIZE all layers
        optimize_all_layers(spark, base_path)
        
        # 5. VACUUM (optional - uncomment to run, uses 7-day retention)
        # vacuum_all_layers(spark, base_path, retention_hours=168)
        
        print("\n" + "=" * 60)
        print("âœ“ PHASE 5 COMPLETE")
        print("=" * 60)
        
    except Exception as e:
        print(f"âœ— Error: {e}")
        raise
    finally:
        spark.stop()


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Delta Lake Features")
    parser.add_argument("--time-travel", action="store_true", help="Run time travel demo only")
    parser.add_argument("--acid", action="store_true", help="Run ACID demo only")
    parser.add_argument("--optimize", action="store_true", help="Run OPTIMIZE only")
    parser.add_argument("--vacuum", action="store_true", help="Run VACUUM (use with caution)")
    parser.add_argument("--retention", type=float, default=168, help="VACUUM retention hours")
    args = parser.parse_args()
    
    use_minio = os.getenv("USE_MINIO", "false").lower() == "true"
    spark, base_path = create_spark_session(app_name="DeltaLakeFeatures", use_minio=use_minio)
    
    try:
        if args.time_travel:
            demonstrate_time_travel(spark, base_path)
        elif args.acid:
            import tempfile
            with tempfile.TemporaryDirectory() as tmpdir:
                demonstrate_acid_transactions(spark, tmpdir)
        elif args.optimize:
            optimize_all_layers(spark, base_path)
        elif args.vacuum:
            vacuum_all_layers(spark, base_path, args.retention)
        else:
            # Run full demo
            paths = get_table_paths(base_path)
            demonstrate_time_travel(spark, base_path)
            import tempfile
            with tempfile.TemporaryDirectory() as tmpdir:
                demonstrate_acid_transactions(spark, tmpdir)
            try:
                tx_path = f"{paths['bronze']}/transactions"
                demonstrate_schema_evolution(
                    spark, tx_path,
                    new_columns={"data_quality_score": 100, "is_verified": False}
                )
            except Exception:
                print("Schema evolution skipped")
            optimize_all_layers(spark, base_path)
            print("\nâœ“ Delta Lake features demo complete")
    finally:
        spark.stop()

